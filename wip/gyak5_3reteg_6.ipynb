{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biomag_labeled_1 = sio.loadmat('data_labeled_5.mat')\n",
    "biomag_labeled_1 = sio.loadmat(r\"D:\\python_project\\wip\\data_labeled_for_py_1_2.mat\")\n",
    "\n",
    "trX, trY, teX, teY, vaX, vaY = biomag_labeled_1['x_train'], biomag_labeled_1['y_train'],\\\n",
    "                               biomag_labeled_1['y_test'], biomag_labeled_1['x_test'],\\\n",
    "                               biomag_labeled_1['x_val'], biomag_labeled_1['y_val'] \n",
    "\n",
    "epochs = 50\n",
    "alpha = 1.0\n",
    "batchsize = 100\n",
    "\n",
    "momentum = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x_train', (2094, 254), 'double'),\n ('y_train', (1, 2094), 'double'),\n ('x_test', (206, 254), 'double'),\n ('y_test', (206, 1), 'uint8'),\n ('x_val', (317, 254), 'double'),\n ('y_val', (317, 12), 'uint8')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sio.whosmat(r\"D:\\python_project\\wip\\data_labeled_for_py_1_2.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2094,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY=np.reshape(trY,2094)\n",
    "trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 254])\n",
    "Y = tf.placeholder(\"float\", [None, 12])\n",
    "\n",
    "rbm_w = tf.placeholder(\"float\", [254, 500])\n",
    "rbm_vb = tf.placeholder(\"float\", [254])\n",
    "rbm_hb = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h0 = tf.nn.sigmoid(tf.matmul(X, rbm_w) + rbm_hb)\n",
    "v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(rbm_w)) + rbm_vb)\n",
    "h1 = tf.nn.sigmoid(tf.matmul(v1, rbm_w) + rbm_hb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(X), h0)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "\n",
    "update_w = rbm_w + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(X)[0])\n",
    "update_vb = rbm_vb + alpha * tf.reduce_mean(X - v1, 0)\n",
    "update_hb = rbm_hb + alpha * tf.reduce_mean(h0 - h1, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(X, rbm_w) + rbm_hb)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w)) + rbm_vb)\n",
    "\n",
    "#define MSE\n",
    "err = X - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.124251075\nMSE:  0.074030735\n0.018301835\nMSE:  0.018232886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01798589\nMSE:  0.017976513\n0.017960126\nMSE:  0.017953912\n0.0179548\nMSE:  0.017949054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017953089\nMSE:  0.017947452\n0.017952353\nMSE:  0.017946748\n0.017951949\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017946353\n0.017951684\nMSE:  0.017946092\n0.017951485\nMSE:  0.017945891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01795132\nMSE:  0.017945727\n0.017951177\nMSE:  0.017945582\n0.017951053\nMSE:  0.017945454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950937\nMSE:  0.017945336\n0.017950831\nMSE:  0.017945226\n0.017950734\nMSE:  0.017945128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950643\nMSE:  0.017945034\n0.017950557\nMSE:  0.017944949\n0.017950477\nMSE:  0.017944865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0179504\nMSE:  0.01794479\n0.017950332\nMSE:  0.01794472\n0.017950265\nMSE:  0.01794465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950203\nMSE:  0.01794459\n0.017950142\nMSE:  0.017944526\n0.017950088\nMSE:  0.017944466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950032\nMSE:  0.017944412\n0.017949982\nMSE:  0.017944358\n0.017949931\nMSE:  0.017944308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949883\nMSE:  0.01794426\n0.017949836\nMSE:  0.017944211\n0.017949792\nMSE:  0.017944166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949747\nMSE:  0.017944118\n0.017949706\nMSE:  0.017944075\n0.017949661\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.01794403\n0.017949618\nMSE:  0.017943988\n0.01794958\nMSE:  0.017943947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949536\nMSE:  0.017943904\n0.017949494\nMSE:  0.017943861\n0.017949456\nMSE:  0.01794382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949414\nMSE:  0.017943777\n0.01794937\nMSE:  0.017943736\n0.017949332\nMSE:  0.017943693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01794929\nMSE:  0.017943652\n0.01794925\nMSE:  0.017943608\n0.017949207\nMSE:  0.017943565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949166\nMSE:  0.017943522\n0.017949123\nMSE:  0.017943477\n0.017949082"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017943434\n0.017949037\nMSE:  0.01794339\n0.01794899\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017943345\n"
     ]
    }
   ],
   "source": [
    "#init weights and biases\n",
    "n_w = np.zeros([254, 500], np.float32)\n",
    "n_vb = np.zeros([254], np.float32)\n",
    "n_hb = np.zeros([500], np.float32)\n",
    "o_w = np.zeros([254, 500], np.float32)\n",
    "o_vb = np.zeros([254], np.float32)\n",
    "o_hb = np.zeros([500], np.float32)\n",
    "\n",
    "#training\n",
    "for e in range(epochs):\n",
    "    print(sess.run(err_sum, feed_dict={X: trX, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb}))\n",
    "    for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "        batch = trX[start:end]\n",
    "        n_w = sess.run(update_w, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        n_vb = sess.run(update_vb, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        n_hb = sess.run(update_hb, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        o_w = n_w\n",
    "        o_vb = n_vb\n",
    "        o_hb = n_hb\n",
    "        if start % 2000 == 0:\n",
    "            print(\"MSE: \", sess.run(err_sum, feed_dict={X: trX, rbm_w: n_w, rbm_vb: n_vb, rbm_hb: n_hb}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2nd layer\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24987753\nMSE:  6.3008365e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  6.3008365e-08\n"
     ]
    }
   ],
   "source": [
    "rbm_w2 = tf.placeholder(\"float\", [500, 500])\n",
    "rbm_vb2 = tf.placeholder(\"float\", [500])\n",
    "rbm_hb2 = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h2 = tf.nn.sigmoid(tf.matmul(h0, rbm_w2) + rbm_hb2)\n",
    "v2 = tf.nn.sigmoid(tf.matmul(h2, tf.transpose(rbm_w2)) + rbm_vb2)\n",
    "h3 = tf.nn.sigmoid(tf.matmul(v2, rbm_w2) + rbm_hb2)\n",
    "\n",
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(h0), h2)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v2), h3)\n",
    "\n",
    "update_w = rbm_w2 + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(h1)[0])\n",
    "update_vb = rbm_vb2 + alpha * tf.reduce_mean(h0 - v2, 0)\n",
    "update_hb = rbm_hb2 + alpha * tf.reduce_mean(h2 - h3, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(h0, rbm_w2) + rbm_hb2)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w2)) + rbm_vb2)\n",
    "\n",
    "#define MSE\n",
    "#X = tf.placeholder(\"float\", [None, 784])          #error dimension fix  -s -k  | X-et kivenni\n",
    "err = h0 - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "n_w_2 = np.zeros([500, 500], np.float32)\n",
    "n_vb_2 = np.zeros([500], np.float32)\n",
    "n_hb_2 = np.zeros([500], np.float32)\n",
    "o_w_2 = np.zeros([500, 500], np.float32)\n",
    "o_vb_2 = np.zeros([500], np.float32)\n",
    "o_hb_2 = np.zeros([500], np.float32)\n",
    "\n",
    "\n",
    "#training\n",
    "print(sess.run(err_sum, feed_dict={rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2, X: trX, rbm_hb : o_hb, rbm_w: o_w}))\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch = trX[start:end]\n",
    "    n_w_2 = sess.run(update_w, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                          rbm_vb: o_vb})\n",
    "    n_vb_2 = sess.run(update_vb, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                            rbm_vb: o_vb})\n",
    "    n_hb_2 = sess.run(update_hb, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                            rbm_vb: o_vb})\n",
    "    o_w_2 = n_w_2\n",
    "    o_vb_2 = n_vb_2\n",
    "    o_hb_2 = n_hb_2\n",
    "    if start % 1000 == 0:\n",
    "        print(\"MSE: \", sess.run(err_sum, feed_dict={rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2, X: trX, rbm_hb : o_hb, rbm_w: o_w}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3nd layer\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005170225\nMSE:  0.004894533\nMSE:  1.5767986e-05\n"
     ]
    }
   ],
   "source": [
    "rbm_w3 = tf.placeholder(\"float\", [500, 500])\n",
    "rbm_vb3 = tf.placeholder(\"float\", [500])\n",
    "rbm_hb3 = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h4 = tf.nn.sigmoid(tf.matmul(h2, rbm_w3) + rbm_hb3)\n",
    "v3 = tf.nn.sigmoid(tf.matmul(h4, tf.transpose(rbm_w3)) + rbm_vb3)\n",
    "h5 = tf.nn.sigmoid(tf.matmul(v3, rbm_w3) + rbm_hb3)\n",
    "\n",
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(h2), h4)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v3), h5)\n",
    "\n",
    "update_w = rbm_w3 + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(h2)[0])\n",
    "update_vb = rbm_vb3 + alpha * tf.reduce_mean(h2 - v3, 0)\n",
    "update_hb = rbm_hb3 + alpha * tf.reduce_mean(h4 - h5, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(h2, rbm_w3) + rbm_hb3)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w3)) + rbm_vb3)\n",
    "\n",
    "#define MSE\n",
    "#X = tf.placeholder(\"float\", [None, 784])          #error dimension fix  -s -k  | X-et kivenni\n",
    "err = h2 - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "n_w_3 = np.zeros([500, 500], np.float32)\n",
    "n_vb_3 = np.zeros([500], np.float32)\n",
    "n_hb_3 = np.zeros([500], np.float32)\n",
    "o_w_3 = np.zeros([500, 500], np.float32)\n",
    "o_vb_3 = np.zeros([500], np.float32)\n",
    "o_hb_3 = np.zeros([500], np.float32)\n",
    "\n",
    "\n",
    "#training\n",
    "print(sess.run(err_sum, feed_dict={rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3, X: trX,\n",
    "                                   rbm_hb2 : o_hb_2, rbm_w2: o_w_2,\n",
    "                                   rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb}))\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch = trX[start:end]\n",
    "    n_w_3 = sess.run(update_w, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    n_vb_3 = sess.run(update_vb, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    n_hb_3 = sess.run(update_hb, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    o_w_3 = n_w_3\n",
    "    o_vb_3 = n_vb_3\n",
    "    o_hb_3 = n_hb_3\n",
    "    if start % 1000 == 0:\n",
    "        print(\"MSE: \", sess.run(err_sum, feed_dict={rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3, X: trX,\n",
    "                                                    rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                                    rbm_hb : o_hb, rbm_w: o_w}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification---------------------------------------------------------------------------------------------\n",
    "#weigths and biases for new output layer\n",
    "out_w = tf.Variable(tf.random_normal([500, 12]))\n",
    "out_b = tf.Variable(tf.random_normal([12]))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# #reformat y train labels\n",
    "trY_formazott = np.zeros([biomag_labeled_1['y_train'].shape[1],12])\n",
    "# #biomag_labeled_1['y_test']\n",
    "for rows in range((biomag_labeled_1['y_train'].shape[1])):\n",
    "    #rows +=1\n",
    "    current_class_label = biomag_labeled_1['y_train'][0,rows] \n",
    "    current_class_label = int(current_class_label)\n",
    "    trY_formazott[rows,current_class_label -1] = 1\n",
    "    # if biomag_labeled_1['y_train'][rows] != 0:\n",
    "    #     trY_formazott[rows] = columns+1 #trY_formazando[rows,columns]\n",
    "trY = trY_formazott\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2094, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trY[567]\n",
    "trY.shape\n",
    "#trY_formazott[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network definition\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.sigmoid(tf.matmul(x, o_w) + o_hb)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_1, out_w) + out_b\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init2 = tf.global_variables_initializer()\n",
    "# Run the initializer\n",
    "sess.run(init2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#biomag_labeled_1['y_test']\n",
    "#Y\n",
    "#trY[355]\n",
    "#biomag_labeled_1['y_train'][2,0]\n",
    "#trY_formazott = np.zeros([biomag_labeled_1['y_train'].shape[0],12])\n",
    "#Y\n",
    "#trY_formazott[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\nStep 0.0, Minibatch Loss= 2.9835, Training Accuracy= 0.140\nStep 5.0, Minibatch Loss= 3.0265, Training Accuracy= 0.090\nStep 10.0, Minibatch Loss= 2.8655, Training Accuracy= 0.120\nStep 15.0, Minibatch Loss= 2.8604, Training Accuracy= 0.120\nEpoch  2\nStep 0.0, Minibatch Loss= 2.7696, Training Accuracy= 0.140\nStep 5.0, Minibatch Loss= 2.8041, Training Accuracy= 0.090\nStep 10.0, Minibatch Loss= 2.6939, Training Accuracy= 0.120"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 15.0, Minibatch Loss= 2.6762, Training Accuracy= 0.120\nEpoch  3\nStep 0.0, Minibatch Loss= 2.5808, Training Accuracy= 0.140\nStep 5.0, Minibatch Loss= 2.6031, Training Accuracy= 0.090\nStep 10.0, Minibatch Loss= 2.5412, Training Accuracy= 0.110\nStep 15.0, Minibatch Loss= 2.5084, Training Accuracy= 0.120"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch  4\nStep 0.0, Minibatch Loss= 2.4159, Training Accuracy= 0.140\nStep 5.0, Minibatch Loss= 2.4211, Training Accuracy= 0.090\nStep 10.0, Minibatch Loss= 2.4063, Training Accuracy= 0.110\nStep 15.0, Minibatch Loss= 2.3568, Training Accuracy= 0.120\nEpoch "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5\nStep 0.0, Minibatch Loss= 2.2733, Training Accuracy= 0.140\nStep 5.0, Minibatch Loss= 2.2580, Training Accuracy= 0.090\nStep 10.0, Minibatch Loss= 2.2904, Training Accuracy= 0.110\nStep 15.0, Minibatch Loss= 2.2243, Training Accuracy= 0.110\nEpoch  6\nStep 0.0, Minibatch Loss= 2.1535, Training Accuracy= 0.130\nStep 5.0, Minibatch Loss= 2.1154, Training Accuracy= 0.080\nStep 10.0, Minibatch Loss= 2.1944, Training Accuracy= 0.110"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 15.0, Minibatch Loss= 2.1130, Training Accuracy= 0.110\nEpoch  7\nStep 0.0, Minibatch Loss= 2.0566, Training Accuracy= 0.120\nStep 5.0, Minibatch Loss= 1.9947, Training Accuracy= 0.080\nStep 10.0, Minibatch Loss= 2.1181, Training Accuracy= 0.090\nStep 15.0, Minibatch Loss= 2.0238, Training Accuracy= 0.110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8\nStep 0.0, Minibatch Loss= 1.9817, Training Accuracy= 0.140\nStep 5.0, Minibatch Loss= 1.8968, Training Accuracy= 0.120\nStep 10.0, Minibatch Loss= 2.0605, Training Accuracy= 0.370\nStep 15.0, Minibatch Loss= 1.9559, Training Accuracy= 0.500\nEpoch  9\nStep 0.0, Minibatch Loss= 1.9265, Training Accuracy= 0.460\nStep 5.0, Minibatch Loss= 1.8206, Training Accuracy= 0.570\nStep 10.0, Minibatch Loss= 2.0189, Training Accuracy= 0.450\nStep 15.0, Minibatch Loss= 1.9066, Training Accuracy= 0.500\nEpoch  10\nStep 0.0, Minibatch Loss= 1.8873, Training Accuracy= 0.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5.0, Minibatch Loss= 1.7634, Training Accuracy= 0.570\nStep 10.0, Minibatch Loss= 1.9899, Training Accuracy= 0.450\nStep 15.0, Minibatch Loss= 1.8723, Training Accuracy= 0.500\n"
     ]
    }
   ],
   "source": [
    "#Training top ANN layer\n",
    "for epochs_2 in range(10):\n",
    "    print(\"Epoch \",epochs_2 +1)\n",
    "    for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "        batch_x = trX[start:end]\n",
    "        batch_y = trY[start:end]\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if start % 500 == 0:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Step \" + str(start/batchsize) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "#batch_x.shape\n",
    "#trY_formazott.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-117117a8fc81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     batch_size=batchsize, shuffle=False)\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use the Estimator 'evaluate' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#biomag_labeled_1['y_train'].shape[0]\n",
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn_test = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'val_data_x': vaX}, y=vaY,\n",
    "    batch_size=batchsize, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative rbm calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative rbm calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup session variables\n",
    "n_classes = trY.shape[1]\n",
    "n = trX.shape[1]\n",
    "dbn.sizes = tf.placeholder(n,rbm_hb.shape)\n",
    "n_rbm = enumerate(dbn.sizes)\n",
    "u = n_rbm\n",
    "\n",
    "vis_size = dbn.sizes[0]\n",
    "hid_size = dbn.sizes[1]\n",
    "\n",
    "#initialize bias and weights for class vector                dbnsetup.m 94\n",
    "rbm.U = weights(hid_size,n_classes)\n",
    "rbm.vU = np.zeros(shape=(hid_size,n_classes))\n",
    "\n",
    "rbm.d = np.zeros(shape=(n_classes,1))\n",
    "rbm.vd = np.zeros(shape=(n_classes,1))\n",
    "\n",
    "\n",
    "rbm.W = weights(hid_size,vis_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#weight initialization\n",
    "\n",
    "def weights(m,n):                                                       #dbmsetup.m line 154\n",
    "    # % initilize weigts from uniform distribution. As described in\n",
    "    # % Learning Algorithms for the Classification Restricted Boltzmann\n",
    "    # % machine\n",
    "    M = max(m,n)\n",
    "    interval_max = M**(-0.5)\n",
    "    interval_min = -interval_max\n",
    "    weights = interval_min + np.multiply(interval_max-interval_min,np.random.uniform(size=(m,n))) \n",
    "    \n",
    "    # assert(max(weights) <= interval_max)\n",
    "    # assert(min(weights) >= interval_min)\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "[p_y_given_x, F] = rbmpygivenx(rbm,x,'train')\n",
    "\n",
    "\n",
    "#RBMPYGIVENX calculates class probabilities [p(y|x)]\n",
    "\n",
    "n_samples = trX.shape[1]          #X parameters (here: 254)         #possibly 1 row and 1 dim. if batch = 1\n",
    "cwx = (n_w_3*trX[:,i]) + n_hb_3     # -ns       #top layer visible-hidden weights * current entity's features + hidden bias\n",
    "#matlab n_w_3 =? rbm.W  (rbm.W = visible - hidden weights          -ns\n",
    "\n",
    "#dropout not used\n",
    "#rbm.hidden_mask = (rbm.rand(size(n_hidden,opts.batchsize)) > rbm.dropout_hidden);\n",
    "\n",
    "rbm_vU = w_positive_grad * alpha + momentum * rbm_vW\n",
    "rbm_U =  rbm_U + rbm_vU\n",
    "\n",
    "F = tf.placeholder(None)\n",
    "F = np.transpose(rbm_u,(0, 2, 1)) + cwx       # -ns\n",
    "\n",
    "class_log_prob = np.zeros(n_samples, n_classes)\n",
    "for y in range(n_classes):\n",
    "    class_log_prob[:,y] = sess.run(sum(tf.nn.softplus(F[:,:,y]),1) + rbm_d[y])    # -missing laber layer\n",
    "\n",
    "# for i in class_log_prob[:]:\n",
    "#     class_prob[0,i] = (class_log_prob - max(class_log_prob)      \n",
    "\n",
    "class_prob = np.exp(np.subtract(class_log_prob,max(class_log_prob)))     #  source: rbmpygivenx.m 25 line  class_log_prob[:] - max(class_log_prob)\n",
    "\n",
    "class_prob = class_prob/sum(class_prob)\n",
    "\n",
    "[p_y_given_x, F] = class_prob,F                            #  source: rbmdiscriminative.m 49 line\n",
    "\n",
    "F_sigm = tf.nn.sigmoid(F)\n",
    "\n",
    "F_sigm_prob = tf.placeholder(F_sigm.shape)\n",
    "\n",
    "\n",
    "for c in n_classes:\n",
    "    F_sigm_prob = np.matmul(F_sigm[:,:,c],np.transpose(p_y_given_x[:,c]))\n",
    "    \n",
    "#init grads\n",
    "dw = tf.zeros(n_w_3.shape)\n",
    "#du = tf.zeros(.shape)             #-ns label - hid weights\n",
    "dc = tf.zeros(n_hb_3.shape)\n",
    "\n",
    "class_labels = ey.index(max(ey))             #      -ns declare ey\n",
    "\n",
    "for c in n_classes:\n",
    "    bin_idx = class_labels == c\n",
    "    lin_idx = (c == class_labels)\n",
    "    \n",
    "    a = F_sigm[:,lin_idx,c] * x[lin_idx,:]           #    -ns  one-hot matlab vs python index problem\n",
    "    b = F_sigm_prob[:,:,c]*x\n",
    "    dw = dw + a-b\n",
    "    \n",
    "    #du grad\n",
    "    du[:,c] = sum(F_sigm[:,bin_idx,c],1) - sum(F_sigm_prob[:,:,c],1)\n",
    "    #dc grad\n",
    "    dc = dc + sum(F_sigm[:,bin_idx,c],1) - sum(F_sigm_prob[:,:,c],1)\n",
    "\n",
    "#dd grad\n",
    "dd = np.transposesum((ey - p_y_given_x,0))\n",
    "\n",
    "\n",
    "dw = dw / batchsize\n",
    "db = tf.zeros(n_vb_3.shape)\n",
    "dc = dc / batchsize\n",
    "dd = dd / batchsize\n",
    "du = du / batchsize\n",
    "#end of rbmdiscriminative.m\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01778202, -0.01778202, -0.01778202, ..., -0.01778202,\n        -0.01778202, -0.01778202],\n       [-0.01778202, -0.01778202, -0.01778202, ..., -0.01778202,\n        -0.01778202, -0.01778202],\n       [-0.01778202, -0.01778202, -0.01778202, ..., -0.01778202,\n        -0.01778202, -0.01778202],\n       ...,\n       [-0.01778202, -0.01778202, -0.01778202, ..., -0.01778202,\n        -0.01778202, -0.01778202],\n       [-0.01778202, -0.01778202, -0.01778202, ..., -0.01778202,\n        -0.01778202, -0.01778202],\n       [-0.01778202, -0.01778202, -0.01778202, ..., -0.01778202,\n        -0.01778202, -0.01778202]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_w_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Equal_1:0' shape=(?,) dtype=bool>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.zeros(25)\n",
    "# i = 0\n",
    "true_preds = 0\n",
    "\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch_x = trX[start:end]\n",
    "    batch_y = trY[start:end]\n",
    "    pred = sess.run(correct_pred, feed_dict={X: batch_x, Y: batch_y})\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == True:\n",
    "            true_preds = true_preds + 1\n",
    "    # predictions[i] = pred\n",
    "    # i = i + 1\n",
    "    \n",
    "    \n",
    "    #predictions[i] = pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002 / 2094\n"
     ]
    }
   ],
   "source": [
    "# true_preds = 0\n",
    "# for i in range(len(pred)):\n",
    "#     if pred[i] == True:\n",
    "#         true_preds = true_preds + 1\n",
    "        \n",
    "#true_preds = true_preds\n",
    "print(true_preds, '/',len(trX))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = true_preds\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred.shape\n",
    "n = trX.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

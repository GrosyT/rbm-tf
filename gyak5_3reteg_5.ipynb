{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biomag_labeled_1 = sio.loadmat('data_labeled_5.mat')\n",
    "biomag_labeled_1 = sio.loadmat(r\"D:\\python_project\\wip\\baseline\\data_labeled_for_py_baseline1_1.mat\")\n",
    "\n",
    "trX, trY, teX, teY = biomag_labeled_1['x_train'], biomag_labeled_1['y_train'],\\\n",
    "                     biomag_labeled_1['y_test'], biomag_labeled_1['x_test']\n",
    "\n",
    "epochs = 50\n",
    "alpha = 1.0\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 254])\n",
    "Y = tf.placeholder(\"float\", [None, 12])\n",
    "\n",
    "rbm_w = tf.placeholder(\"float\", [254, 500])\n",
    "rbm_vb = tf.placeholder(\"float\", [254])\n",
    "rbm_hb = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h0 = tf.nn.sigmoid(tf.matmul(X, rbm_w) + rbm_hb)\n",
    "v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(rbm_w)) + rbm_vb)\n",
    "h1 = tf.nn.sigmoid(tf.matmul(v1, rbm_w) + rbm_hb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(X), h0)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "\n",
    "update_w = rbm_w + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(X)[0])\n",
    "update_vb = rbm_vb + alpha * tf.reduce_mean(X - v1, 0)\n",
    "update_hb = rbm_hb + alpha * tf.reduce_mean(h0 - h1, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(X, rbm_w) + rbm_hb)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w)) + rbm_vb)\n",
    "\n",
    "#define MSE\n",
    "err = X - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.124251075\nMSE:  0.074030735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018301835\nMSE:  0.018232886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01798589\nMSE:  0.017976513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017960126\nMSE:  0.017953912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0179548\nMSE:  0.017949054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017953089\nMSE:  0.017947452\n0.017952353\nMSE:  0.017946748\n0.017951949\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017946353\n0.017951684\nMSE:  0.017946092\n0.017951485\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017945891\n0.01795132\nMSE:  0.017945727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017951177\nMSE:  0.017945582\n0.017951053\nMSE:  0.017945454\n0.017950937\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017945336\n0.017950831\nMSE:  0.017945226\n0.017950734\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017945128\n0.017950643\nMSE:  0.017945034\n0.017950557\nMSE:  0.017944949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950477\nMSE:  0.017944865\n0.0179504\nMSE:  0.01794479\n0.017950332"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.01794472\n0.017950265\nMSE:  0.01794465\n0.017950203"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.01794459\n0.017950142\nMSE:  0.017944526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950088\nMSE:  0.017944466\n0.017950032\nMSE:  0.017944412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949982\nMSE:  0.017944358\n0.017949931\nMSE:  0.017944308\n0.017949883"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.01794426\n0.017949836\nMSE:  0.017944211\n0.017949792"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017944166\n0.017949747\nMSE:  0.017944118\n0.017949706\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017944075\n0.017949661\nMSE:  0.01794403\n0.017949618\nMSE:  0.017943988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01794958\nMSE:  0.017943947\n0.017949536\nMSE:  0.017943904\n0.017949494"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017943861\n0.017949456\nMSE:  0.01794382\n0.017949414"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017943777\n0.01794937\nMSE:  0.017943736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949332\nMSE:  0.017943693\n0.01794929\nMSE:  0.017943652\n0.01794925\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017943608\n0.017949207\nMSE:  0.017943565\n0.017949166\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017943522\n0.017949123\nMSE:  0.017943477\n0.017949082\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017943434\n0.017949037\nMSE:  0.01794339\n0.01794899"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017943345\n"
     ]
    }
   ],
   "source": [
    "#init weights and biases\n",
    "n_w = np.zeros([254, 500], np.float32)\n",
    "n_vb = np.zeros([254], np.float32)\n",
    "n_hb = np.zeros([500], np.float32)\n",
    "o_w = np.zeros([254, 500], np.float32)\n",
    "o_vb = np.zeros([254], np.float32)\n",
    "o_hb = np.zeros([500], np.float32)\n",
    "\n",
    "#training\n",
    "for e in range(epochs):\n",
    "    print(sess.run(err_sum, feed_dict={X: trX, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb}))\n",
    "    for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "        batch = trX[start:end]\n",
    "        n_w = sess.run(update_w, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        n_vb = sess.run(update_vb, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        n_hb = sess.run(update_hb, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        o_w = n_w\n",
    "        o_vb = n_vb\n",
    "        o_hb = n_hb\n",
    "        if start % 2000 == 0:\n",
    "            print(\"MSE: \", sess.run(err_sum, feed_dict={X: trX, rbm_w: n_w, rbm_vb: n_vb, rbm_hb: n_hb}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2nd layer\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24987753\nMSE:  6.3008365e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  6.3008365e-08\n"
     ]
    }
   ],
   "source": [
    "rbm_w2 = tf.placeholder(\"float\", [500, 500])\n",
    "rbm_vb2 = tf.placeholder(\"float\", [500])\n",
    "rbm_hb2 = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h2 = tf.nn.sigmoid(tf.matmul(h0, rbm_w2) + rbm_hb2)\n",
    "v2 = tf.nn.sigmoid(tf.matmul(h2, tf.transpose(rbm_w2)) + rbm_vb2)\n",
    "h3 = tf.nn.sigmoid(tf.matmul(v2, rbm_w2) + rbm_hb2)\n",
    "\n",
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(h0), h2)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v2), h3)\n",
    "\n",
    "update_w = rbm_w2 + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(h1)[0])\n",
    "update_vb = rbm_vb2 + alpha * tf.reduce_mean(h0 - v2, 0)\n",
    "update_hb = rbm_hb2 + alpha * tf.reduce_mean(h2 - h3, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(h0, rbm_w2) + rbm_hb2)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w2)) + rbm_vb2)\n",
    "\n",
    "#define MSE\n",
    "#X = tf.placeholder(\"float\", [None, 784])          #error dimension fix  -s -k  | X-et kivenni\n",
    "err = h0 - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "n_w_2 = np.zeros([500, 500], np.float32)\n",
    "n_vb_2 = np.zeros([500], np.float32)\n",
    "n_hb_2 = np.zeros([500], np.float32)\n",
    "o_w_2 = np.zeros([500, 500], np.float32)\n",
    "o_vb_2 = np.zeros([500], np.float32)\n",
    "o_hb_2 = np.zeros([500], np.float32)\n",
    "\n",
    "\n",
    "#training\n",
    "print(sess.run(err_sum, feed_dict={rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2, X: trX, rbm_hb : o_hb, rbm_w: o_w}))\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch = trX[start:end]\n",
    "    n_w_2 = sess.run(update_w, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                          rbm_vb: o_vb})\n",
    "    n_vb_2 = sess.run(update_vb, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                            rbm_vb: o_vb})\n",
    "    n_hb_2 = sess.run(update_hb, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                            rbm_vb: o_vb})\n",
    "    o_w_2 = n_w_2\n",
    "    o_vb_2 = n_vb_2\n",
    "    o_hb_2 = n_hb_2\n",
    "    if start % 1000 == 0:\n",
    "        print(\"MSE: \", sess.run(err_sum, feed_dict={rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2, X: trX, rbm_hb : o_hb, rbm_w: o_w}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3nd layer\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005170225\nMSE:  0.004894533\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.5767986e-05\n"
     ]
    }
   ],
   "source": [
    "rbm_w3 = tf.placeholder(\"float\", [500, 500])\n",
    "rbm_vb3 = tf.placeholder(\"float\", [500])\n",
    "rbm_hb3 = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h4 = tf.nn.sigmoid(tf.matmul(h2, rbm_w3) + rbm_hb3)\n",
    "v3 = tf.nn.sigmoid(tf.matmul(h4, tf.transpose(rbm_w3)) + rbm_vb3)\n",
    "h5 = tf.nn.sigmoid(tf.matmul(v3, rbm_w3) + rbm_hb3)\n",
    "\n",
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(h2), h4)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v3), h5)\n",
    "\n",
    "update_w = rbm_w3 + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(h2)[0])\n",
    "update_vb = rbm_vb3 + alpha * tf.reduce_mean(h2 - v3, 0)\n",
    "update_hb = rbm_hb3 + alpha * tf.reduce_mean(h4 - h5, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(h2, rbm_w3) + rbm_hb3)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w3)) + rbm_vb3)\n",
    "\n",
    "#define MSE\n",
    "#X = tf.placeholder(\"float\", [None, 784])          #error dimension fix  -s -k  | X-et kivenni\n",
    "err = h2 - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "n_w_3 = np.zeros([500, 500], np.float32)\n",
    "n_vb_3 = np.zeros([500], np.float32)\n",
    "n_hb_3 = np.zeros([500], np.float32)\n",
    "o_w_3 = np.zeros([500, 500], np.float32)\n",
    "o_vb_3 = np.zeros([500], np.float32)\n",
    "o_hb_3 = np.zeros([500], np.float32)\n",
    "\n",
    "\n",
    "#training\n",
    "print(sess.run(err_sum, feed_dict={rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3, X: trX,\n",
    "                                   rbm_hb2 : o_hb_2, rbm_w2: o_w_2,\n",
    "                                   rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb}))\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch = trX[start:end]\n",
    "    n_w_3 = sess.run(update_w, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    n_vb_3 = sess.run(update_vb, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    n_hb_3 = sess.run(update_hb, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    o_w_3 = n_w_3\n",
    "    o_vb_3 = n_vb_3\n",
    "    o_hb_3 = n_hb_3\n",
    "    if start % 1000 == 0:\n",
    "        print(\"MSE: \", sess.run(err_sum, feed_dict={rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3, X: trX,\n",
    "                                                    rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                                    rbm_hb : o_hb, rbm_w: o_w}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0.0, Minibatch Loss= 2.7881, Training Accuracy= 0.030\nStep 1.0, Minibatch Loss= 2.8270, Training Accuracy= 0.000\nStep 2.0, Minibatch Loss= 2.7894, Training Accuracy= 0.030\nStep 3.0, Minibatch Loss= 2.8206, Training Accuracy= 0.010\nStep 4.0, Minibatch Loss= 2.8217, Training Accuracy= 0.000\nStep 5.0, Minibatch Loss= 2.7259, Training Accuracy= 0.010\nStep 6.0, Minibatch Loss= 2.7487, Training Accuracy= 0.000\nStep 7.0, Minibatch Loss= 2.6823, Training Accuracy= 0.020\nStep 8.0, Minibatch Loss= 2.7013, Training Accuracy= 0.020\nStep 9.0, Minibatch Loss= 2.6603, Training Accuracy= 0.000\nStep 10.0, Minibatch Loss= 2.7848, Training Accuracy= 0.020\nStep 11.0, Minibatch Loss= 2.8369, Training Accuracy= 0.010"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 12.0, Minibatch Loss= 2.6945, Training Accuracy= 0.020\nStep 13.0, Minibatch Loss= 2.6563, Training Accuracy= 0.020\nStep 14.0, Minibatch Loss= 2.5566, Training Accuracy= 0.010\nStep 15.0, Minibatch Loss= 2.6249, Training Accuracy= 0.020\nStep 16.0, Minibatch Loss= 2.6557, Training Accuracy= 0.020\nStep 17.0, Minibatch Loss= 2.6501, Training Accuracy= 0.000\nStep 18.0, Minibatch Loss= 2.6064, Training Accuracy= 0.010\nStep 19.0, Minibatch Loss= 2.6095, Training Accuracy= 0.030\n"
     ]
    }
   ],
   "source": [
    "#Classification---------------------------------------------------------------------------------------------\n",
    "#weigths and biases for new output layer\n",
    "out_w = tf.Variable(tf.random_normal([500, 12]))\n",
    "out_b = tf.Variable(tf.random_normal([12]))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# #reformat y train labels\n",
    "trY_formazott = np.zeros([biomag_labeled_1['y_train'].shape[0],12])\n",
    "# #biomag_labeled_1['y_test']\n",
    "for rows in range((biomag_labeled_1['y_train'].shape[0])):\n",
    "    #rows +=1\n",
    "    current_class_label = biomag_labeled_1['y_train'][rows,0] \n",
    "    current_class_label = int(current_class_label)\n",
    "    trY_formazott[rows,current_class_label -1] = 1\n",
    "    # if biomag_labeled_1['y_train'][rows] != 0:\n",
    "    #     trY_formazott[rows] = columns+1 #trY_formazando[rows,columns]\n",
    "trY = trY_formazott\n",
    "\n",
    "#network definition\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.sigmoid(tf.matmul(x, o_w) + o_hb)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_1, out_w) + out_b\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init2 = tf.global_variables_initializer()\n",
    "# Run the initializer\n",
    "sess.run(init2)\n",
    "\n",
    "\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "  batch_x = trX[start:end]\n",
    "  batch_y = trY[start:end]\n",
    "  sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "  if start % 100 == 0:\n",
    "    loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "    print(\"Step \" + str(start/batchsize) + \", Minibatch Loss= \" + \\\n",
    "      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "      \"{:.3f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#biomag_labeled_1['y_test']\n",
    "#Y\n",
    "#trY[355]\n",
    "#biomag_labeled_1['y_train'][2,0]\n",
    "#trY_formazott = np.zeros([biomag_labeled_1['y_train'].shape[0],12])\n",
    "#Y\n",
    "trY_formazott[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2094, 12)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch_x.shape\n",
    "trY_formazott.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomag_labeled_1['y_train'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

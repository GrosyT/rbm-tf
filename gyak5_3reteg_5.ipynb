{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biomag_labeled_1 = sio.loadmat('data_labeled_5.mat')\n",
    "biomag_labeled_1 = sio.loadmat(r\"D:\\python_project\\wip\\data_labeled_for_py_1_2.mat\")\n",
    "\n",
    "trX, trY, teX, teY, vaX, vaY = biomag_labeled_1['x_train'], biomag_labeled_1['y_train'],\\\n",
    "                               biomag_labeled_1['y_test'], biomag_labeled_1['x_test'],\\\n",
    "                               biomag_labeled_1['x_val'], biomag_labeled_1['y_val'] \n",
    "\n",
    "epochs = 50\n",
    "alpha = 1.0\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x_train', (2094, 254), 'double'),\n ('y_train', (1, 2094), 'double'),\n ('x_test', (206, 254), 'double'),\n ('y_test', (206, 1), 'uint8'),\n ('x_val', (317, 254), 'double'),\n ('y_val', (317, 12), 'uint8')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sio.whosmat(r\"D:\\python_project\\wip\\data_labeled_for_py_1_2.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2094,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY=np.reshape(trY,2094)\n",
    "trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 254])\n",
    "Y = tf.placeholder(\"float\", [None, 12])\n",
    "\n",
    "rbm_w = tf.placeholder(\"float\", [254, 500])\n",
    "rbm_vb = tf.placeholder(\"float\", [254])\n",
    "rbm_hb = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h0 = tf.nn.sigmoid(tf.matmul(X, rbm_w) + rbm_hb)\n",
    "v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(rbm_w)) + rbm_vb)\n",
    "h1 = tf.nn.sigmoid(tf.matmul(v1, rbm_w) + rbm_hb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(X), h0)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "\n",
    "update_w = rbm_w + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(X)[0])\n",
    "update_vb = rbm_vb + alpha * tf.reduce_mean(X - v1, 0)\n",
    "update_hb = rbm_hb + alpha * tf.reduce_mean(h0 - h1, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(X, rbm_w) + rbm_hb)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w)) + rbm_vb)\n",
    "\n",
    "#define MSE\n",
    "err = X - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.124251075\nMSE:  0.074030735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018301835\nMSE:  0.018232886\n0.01798589"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017976513\n0.017960126\nMSE:  0.017953912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0179548\nMSE:  0.017949054\n0.017953089\nMSE:  0.017947452\n0.017952353"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017946748\n0.017951949\nMSE:  0.017946353\n0.017951684\nMSE:  0.017946092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017951485\nMSE:  0.017945891\n0.01795132\nMSE:  0.017945727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017951177\nMSE:  0.017945582\n0.017951053\nMSE:  0.017945454\n0.017950937\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017945336\n0.017950831\nMSE:  0.017945226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950734\nMSE:  0.017945128\n0.017950643\nMSE:  0.017945034\n0.017950557"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017944949\n0.017950477\nMSE:  0.017944865\n0.0179504\nMSE:  0.01794479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950332\nMSE:  0.01794472\n0.017950265\nMSE:  0.01794465\n0.017950203"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.01794459\n0.017950142\nMSE:  0.017944526\n0.017950088\nMSE:  0.017944466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017950032\nMSE:  0.017944412\n0.017949982\nMSE:  0.017944358\n0.017949931\nMSE: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.017944308\n0.017949883\nMSE:  0.01794426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949836\nMSE:  0.017944211\n0.017949792\nMSE:  0.017944166\n0.017949747\nMSE:  0.017944118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949706\nMSE:  0.017944075\n0.017949661\nMSE:  0.01794403\n0.017949618\nMSE:  0.017943988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01794958\nMSE:  0.017943947\n0.017949536\nMSE:  0.017943904\n0.017949494\nMSE:  0.017943861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949456\nMSE:  0.01794382\n0.017949414\nMSE:  0.017943777\n0.01794937\nMSE:  0.017943736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949332\nMSE:  0.017943693\n0.01794929\nMSE:  0.017943652\n0.01794925"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMSE:  0.017943608\n0.017949207\nMSE:  0.017943565\n0.017949166\nMSE:  0.017943522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017949123\nMSE:  0.017943477\n0.017949082\nMSE:  0.017943434\n0.017949037\nMSE:  0.01794339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01794899\nMSE:  0.017943345\n"
     ]
    }
   ],
   "source": [
    "#init weights and biases\n",
    "n_w = np.zeros([254, 500], np.float32)\n",
    "n_vb = np.zeros([254], np.float32)\n",
    "n_hb = np.zeros([500], np.float32)\n",
    "o_w = np.zeros([254, 500], np.float32)\n",
    "o_vb = np.zeros([254], np.float32)\n",
    "o_hb = np.zeros([500], np.float32)\n",
    "\n",
    "#training\n",
    "for e in range(epochs):\n",
    "    print(sess.run(err_sum, feed_dict={X: trX, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb}))\n",
    "    for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "        batch = trX[start:end]\n",
    "        n_w = sess.run(update_w, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        n_vb = sess.run(update_vb, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        n_hb = sess.run(update_hb, feed_dict={X: batch, rbm_w: o_w, rbm_vb: o_vb, rbm_hb: o_hb})\n",
    "        o_w = n_w\n",
    "        o_vb = n_vb\n",
    "        o_hb = n_hb\n",
    "        if start % 2000 == 0:\n",
    "            print(\"MSE: \", sess.run(err_sum, feed_dict={X: trX, rbm_w: n_w, rbm_vb: n_vb, rbm_hb: n_hb}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2nd layer\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24987753\nMSE:  6.3008365e-08\nMSE:  6.3008365e-08\n"
     ]
    }
   ],
   "source": [
    "rbm_w2 = tf.placeholder(\"float\", [500, 500])\n",
    "rbm_vb2 = tf.placeholder(\"float\", [500])\n",
    "rbm_hb2 = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h2 = tf.nn.sigmoid(tf.matmul(h0, rbm_w2) + rbm_hb2)\n",
    "v2 = tf.nn.sigmoid(tf.matmul(h2, tf.transpose(rbm_w2)) + rbm_vb2)\n",
    "h3 = tf.nn.sigmoid(tf.matmul(v2, rbm_w2) + rbm_hb2)\n",
    "\n",
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(h0), h2)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v2), h3)\n",
    "\n",
    "update_w = rbm_w2 + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(h1)[0])\n",
    "update_vb = rbm_vb2 + alpha * tf.reduce_mean(h0 - v2, 0)\n",
    "update_hb = rbm_hb2 + alpha * tf.reduce_mean(h2 - h3, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(h0, rbm_w2) + rbm_hb2)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w2)) + rbm_vb2)\n",
    "\n",
    "#define MSE\n",
    "#X = tf.placeholder(\"float\", [None, 784])          #error dimension fix  -s -k  | X-et kivenni\n",
    "err = h0 - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "n_w_2 = np.zeros([500, 500], np.float32)\n",
    "n_vb_2 = np.zeros([500], np.float32)\n",
    "n_hb_2 = np.zeros([500], np.float32)\n",
    "o_w_2 = np.zeros([500, 500], np.float32)\n",
    "o_vb_2 = np.zeros([500], np.float32)\n",
    "o_hb_2 = np.zeros([500], np.float32)\n",
    "\n",
    "\n",
    "#training\n",
    "print(sess.run(err_sum, feed_dict={rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2, X: trX, rbm_hb : o_hb, rbm_w: o_w}))\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch = trX[start:end]\n",
    "    n_w_2 = sess.run(update_w, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                          rbm_vb: o_vb})\n",
    "    n_vb_2 = sess.run(update_vb, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                            rbm_vb: o_vb})\n",
    "    n_hb_2 = sess.run(update_hb, feed_dict={X: batch, rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,  rbm_hb : o_hb, rbm_w: o_w,\n",
    "                                            rbm_vb: o_vb})\n",
    "    o_w_2 = n_w_2\n",
    "    o_vb_2 = n_vb_2\n",
    "    o_hb_2 = n_hb_2\n",
    "    if start % 1000 == 0:\n",
    "        print(\"MSE: \", sess.run(err_sum, feed_dict={rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2, X: trX, rbm_hb : o_hb, rbm_w: o_w}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3nd layer\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005170225\nMSE:  0.004894533\nMSE:  1.5767986e-05\n"
     ]
    }
   ],
   "source": [
    "rbm_w3 = tf.placeholder(\"float\", [500, 500])\n",
    "rbm_vb3 = tf.placeholder(\"float\", [500])\n",
    "rbm_hb3 = tf.placeholder(\"float\", [500])\n",
    "\n",
    "h4 = tf.nn.sigmoid(tf.matmul(h2, rbm_w3) + rbm_hb3)\n",
    "v3 = tf.nn.sigmoid(tf.matmul(h4, tf.transpose(rbm_w3)) + rbm_vb3)\n",
    "h5 = tf.nn.sigmoid(tf.matmul(v3, rbm_w3) + rbm_hb3)\n",
    "\n",
    "#grad calculation\n",
    "w_positive_grad = tf.matmul(tf.transpose(h2), h4)\n",
    "w_negative_grad = tf.matmul(tf.transpose(v3), h5)\n",
    "\n",
    "update_w = rbm_w3 + alpha * (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(h2)[0])\n",
    "update_vb = rbm_vb3 + alpha * tf.reduce_mean(h2 - v3, 0)\n",
    "update_hb = rbm_hb3 + alpha * tf.reduce_mean(h4 - h5, 0)\n",
    "\n",
    "h_sample = tf.nn.sigmoid(tf.matmul(h2, rbm_w3) + rbm_hb3)\n",
    "v_sample = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(rbm_w3)) + rbm_vb3)\n",
    "\n",
    "#define MSE\n",
    "#X = tf.placeholder(\"float\", [None, 784])          #error dimension fix  -s -k  | X-et kivenni\n",
    "err = h2 - v_sample\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "n_w_3 = np.zeros([500, 500], np.float32)\n",
    "n_vb_3 = np.zeros([500], np.float32)\n",
    "n_hb_3 = np.zeros([500], np.float32)\n",
    "o_w_3 = np.zeros([500, 500], np.float32)\n",
    "o_vb_3 = np.zeros([500], np.float32)\n",
    "o_hb_3 = np.zeros([500], np.float32)\n",
    "\n",
    "\n",
    "#training\n",
    "print(sess.run(err_sum, feed_dict={rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3, X: trX,\n",
    "                                   rbm_hb2 : o_hb_2, rbm_w2: o_w_2,\n",
    "                                   rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb}))\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch = trX[start:end]\n",
    "    n_w_3 = sess.run(update_w, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    n_vb_3 = sess.run(update_vb, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    n_hb_3 = sess.run(update_hb, feed_dict={X: batch, rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3,\n",
    "                                          rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                          rbm_hb : o_hb, rbm_w: o_w, rbm_vb: o_vb})\n",
    "    o_w_3 = n_w_3\n",
    "    o_vb_3 = n_vb_3\n",
    "    o_hb_3 = n_hb_3\n",
    "    if start % 1000 == 0:\n",
    "        print(\"MSE: \", sess.run(err_sum, feed_dict={rbm_w3: o_w_3, rbm_vb3: o_vb_3, rbm_hb3: o_hb_3, X: trX,\n",
    "                                                    rbm_w2: o_w_2, rbm_vb2: o_vb_2, rbm_hb2: o_hb_2,\n",
    "                                                    rbm_hb : o_hb, rbm_w: o_w}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification---------------------------------------------------------------------------------------------\n",
    "#weigths and biases for new output layer\n",
    "out_w = tf.Variable(tf.random_normal([500, 12]))\n",
    "out_b = tf.Variable(tf.random_normal([12]))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# #reformat y train labels\n",
    "trY_formazott = np.zeros([biomag_labeled_1['y_train'].shape[1],12])\n",
    "# #biomag_labeled_1['y_test']\n",
    "for rows in range((biomag_labeled_1['y_train'].shape[1])):\n",
    "    #rows +=1\n",
    "    current_class_label = biomag_labeled_1['y_train'][0,rows] \n",
    "    current_class_label = int(current_class_label)\n",
    "    trY_formazott[rows,current_class_label -1] = 1\n",
    "    # if biomag_labeled_1['y_train'][rows] != 0:\n",
    "    #     trY_formazott[rows] = columns+1 #trY_formazando[rows,columns]\n",
    "trY = trY_formazott\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2094, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trY[567]\n",
    "trY.shape\n",
    "#trY_formazott[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network definition\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.nn.sigmoid(tf.matmul(x, o_w) + o_hb)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_1, out_w) + out_b\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init2 = tf.global_variables_initializer()\n",
    "# Run the initializer\n",
    "sess.run(init2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#biomag_labeled_1['y_test']\n",
    "#Y\n",
    "#trY[355]\n",
    "#biomag_labeled_1['y_train'][2,0]\n",
    "#trY_formazott = np.zeros([biomag_labeled_1['y_train'].shape[0],12])\n",
    "#Y\n",
    "#trY_formazott[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0.0, Minibatch Loss= 1.7293, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8501, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7293, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8500, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7292, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8499, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7291, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8498, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7291, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8496, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7290, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8495, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7289, Training Accuracy= 0.480"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 10.0, Minibatch Loss= 1.8494, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7289, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8493, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7288, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8492, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7288, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8491, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7287, Training Accuracy= 0.480"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 10.0, Minibatch Loss= 1.8490, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7286, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8489, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7286, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8488, Training Accuracy= 0.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0.0, Minibatch Loss= 1.7285, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8487, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7285, Training Accuracy= 0.480"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10.0, Minibatch Loss= 1.8486, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7284, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8485, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7284, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8483, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7283, Training Accuracy= 0.480\nStep 10.0, Minibatch Loss= 1.8482, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7283, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8481, Training Accuracy= 0.460"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 0.0, Minibatch Loss= 1.7282, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8481, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7282, Training Accuracy= 0.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10.0, Minibatch Loss= 1.8480, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7281, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8479, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7281, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8478, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7280, Training Accuracy= 0.470"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 10.0, Minibatch Loss= 1.8477, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7280, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8476, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7279, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8475, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7279, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8474, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7278, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8473, Training Accuracy= 0.460"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStep 0.0, Minibatch Loss= 1.7278, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8472, Training Accuracy= 0.460\nStep 0.0, Minibatch Loss= 1.7277, Training Accuracy= 0.470\nStep 10.0, Minibatch Loss= 1.8471, Training Accuracy= 0.460\n"
     ]
    }
   ],
   "source": [
    "#Training top ANN layer\n",
    "for epochs_2 in range(30):\n",
    "    for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "        batch_x = trX[start:end]\n",
    "        batch_y = trY[start:end]\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if start % 1000 == 0:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Step \" + str(start/batchsize) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "#batch_x.shape\n",
    "#trY_formazott.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-117117a8fc81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     batch_size=batchsize, shuffle=False)\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use the Estimator 'evaluate' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#biomag_labeled_1['y_train'].shape[0]\n",
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn_test = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'val_data_x': vaX}, y=vaY,\n",
    "    batch_size=batchsize, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Equal_1:0' shape=(?,) dtype=bool>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.zeros(25)\n",
    "# i = 0\n",
    "true_preds = 0\n",
    "\n",
    "for start, end in zip(range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "    batch_x = trX[start:end]\n",
    "    batch_y = trY[start:end]\n",
    "    pred = sess.run(correct_pred, feed_dict={X: batch_x, Y: batch_y})\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == True:\n",
    "            true_preds = true_preds + 1\n",
    "    # predictions[i] = pred\n",
    "    # i = i + 1\n",
    "    \n",
    "    \n",
    "    #predictions[i] = pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002 / 100\n"
     ]
    }
   ],
   "source": [
    "# true_preds = 0\n",
    "# for i in range(len(pred)):\n",
    "#     if pred[i] == True:\n",
    "#         true_preds = true_preds + 1\n",
    "        \n",
    "#true_preds = true_preds\n",
    "print(true_preds, '/',len(pred))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = true_preds\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
